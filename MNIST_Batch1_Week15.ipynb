{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n",
      "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n",
      "Python Version: 3\n",
      "TensorFlow Version: 1.3.0\n",
      "Keras Version: 2.1.3\n",
      "GPU Enabled?: False\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 29s - loss: 0.3466 - acc: 0.8899 - val_loss: 0.0734 - val_acc: 0.9769\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.1004 - acc: 0.9683 - val_loss: 0.0508 - val_acc: 0.9823\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.0751 - acc: 0.9766 - val_loss: 0.0380 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      " - 33s - loss: 0.0606 - acc: 0.9809 - val_loss: 0.0332 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      " - 33s - loss: 0.0529 - acc: 0.9835 - val_loss: 0.0306 - val_acc: 0.9897\n",
      "Epoch 6/10\n",
      " - 33s - loss: 0.0471 - acc: 0.9859 - val_loss: 0.0289 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      " - 34s - loss: 0.0428 - acc: 0.9860 - val_loss: 0.0296 - val_acc: 0.9903\n",
      "Epoch 8/10\n",
      " - 34s - loss: 0.0391 - acc: 0.9876 - val_loss: 0.0273 - val_acc: 0.9913\n",
      "Epoch 9/10\n",
      " - 34s - loss: 0.0378 - acc: 0.9882 - val_loss: 0.0245 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      " - 33s - loss: 0.0327 - acc: 0.9895 - val_loss: 0.0258 - val_acc: 0.9923\n",
      "Large CNN Accuracy: 99.23%\n",
      "Large CNN Error: 0.77%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set up dataset specific values\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Load dataset\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# (for MLP only) flatten 28*28 images to a 784 vector for each image\n",
    "#num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "#X_train = x_train.reshape(x_train.shape[0], num_pixels).astype('float32')\n",
    "#X_test = x_test.reshape(x_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# (for CNNs)\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# reshape to be [samples][width][height][channels]\n",
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "pprint(y_train)\n",
    "pprint(y_test)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "from __future__ import absolute_import, print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "#%matplotlib inline\n",
    "\n",
    "print('Python Version: {}'.format(sys.version_info[0]))\n",
    "print('TensorFlow Version: {}'.format(tensorflow.__version__))\n",
    "print('Keras Version: {}'.format(keras.__version__))\n",
    "print('GPU Enabled?: {}'.format(tensorflow.test.gpu_device_name() is not ''))\n",
    "\n",
    "\n",
    "# define the larger model\n",
    "def larger_cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=input_shape, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "large_cnn_model = larger_cnn_model()  \n",
    "    \n",
    "# Fit the model\n",
    "large_cnn_model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test), \n",
    "          epochs=10, batch_size = batch_size, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "large_cnn_scores = large_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Large CNN Accuracy: {:.2f}%\".format(large_cnn_scores[1]*100))\n",
    "print(\"Large CNN Error: {:.2f}%\".format(100-large_cnn_scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
